# Tokenizer 分词器

三大类：

* 古典分词：按空格，标点符号分词
* 拆为单个字符： 拆分为字母和特殊符号
* 基于字词的分词(subword)：把词切分成更小的一块一块的字词，例如，例如：“unfortunately ” = “un ” + “for ” + “tun ” + “ate ” + “ly ”

介绍三种主流的subword方法：**BPE**,**Wordpiece**，**Unigram** ， 参考[huggingface course](https://huggingface.co/learn/llm-course/en/chapter6/5)

## [BPE (Byte Pair Encoding)](../code/tokenizer/bpetokenizer.py)

BPE最初是一种文本压缩算法，后被OpenAI用于GPT模型的分词。它被广泛应用于Transformer模型中，如GPT、GPT-2、RoBERTa、BART和DeBERTa。

### 训练算法原理

BPE从基础词汇表开始，通过学习合并规则来逐步构建更大的词汇表：

1. **初始化**：从语料库中提取所有唯一字符作为基础词汇表
2. **迭代合并**：寻找出现频率最高的相邻字符对进行合并
3. **重复**：持续合并直到达到目标词汇表大小

### 数学描述

给定语料库中的词频统计，BPE算法选择合并规则：

$$\text{merge}(x,y) = \arg\max_{(x,y)} \text{count}(x,y)$$

其中 $\text{count}(x,y)$ 表示字符对 $(x,y)$ 在语料库中的出现频次。

### 分词算法

分词过程遵循训练时学习的合并规则：
1. **归一化**：文本预处理
2. **预分词**：按词边界分割
3. **字符分割**：将词分割为单个字符
4. **应用合并规则**：按训练时的顺序应用合并规则


## [WordPiece](../code/tokenizer/wordpiecetokenizer.py)

WordPiece由Google开发，用于BERT模型的预训练，后被DistilBERT、MobileBERT等模型采用。虽然训练过程与BPE相似，但分词策略有所不同。

### 训练算法原理

WordPiece使用前缀标记（如BERT中的`##`）来标识子词：

1. **初始化**：构建包含所有字符的基础词汇表，非首字符添加`##`前缀
2. **评分计算**：使用不同于BPE的评分函数选择合并对
3. **迭代合并**：选择得分最高的字符对进行合并

### 数学描述

WordPiece的合并准则基于似然性而非简单频率：

$$\text{score}(x,y) = \frac{P(xy)}{P(x) \times P(y)}$$

选择得分最高的字符对进行合并：

$$\text{merge} = \arg\max_{(x,y)} \text{score}(x,y)$$

其中：
- $P(x)$、$P(y)$ 分别表示单个token在语料中的概率
- $P(xy)$ 表示组合token在语料中的概率

### 分词算法

WordPiece采用**最长匹配**策略：
1. 从词的开头寻找词汇表中最长的子词
2. 分割并继续处理剩余部分
3. 如果无法找到匹配，标记为未知词`[UNK]`

### 算法特点

- **似然性驱动**：优先合并能最大化训练数据似然性的字符对
- **前缀标记**：使用`##`等前缀区分词首和词内字符
- **最长匹配**：分词时采用贪心的最长匹配策略

## [Unigram](../code/tokenizer/unigramtokenizer.py)

Unigram算法常与**SentencePiece**结合使用，被应用于AlBERT、T5、mBART、Big Bird和XLNet等模型。

### 训练算法原理

Unigram采用与BPE和WordPiece相反的策略：

1. **初始化**：从较大的候选词汇表开始
2. **损失计算**：计算当前词汇表在语料库上的损失
3. **迭代剪枝**：移除对损失影响最小的token
4. **重复**：直到达到目标词汇表大小

### 数学描述

**Unigram语言模型**假设每个token独立出现，token的概率为：

$$P(\text{token}) = \frac{\text{freq}(\text{token})}{\sum_{\text{all tokens}} \text{freq}(\text{token})}$$

**分词的概率**是所有token概率的乘积：

$$P(\text{tokenization}) = \prod_{i=1}^{n} P(\text{token}_i)$$

**语料库的总损失**为负对数似然：

$$\mathcal{L} = -\sum_{\text{word} \in \text{corpus}} \text{freq}(\text{word}) \times \log P(\text{best\_tokenization}(\text{word}))$$

### 分词算法（Viterbi算法）

使用**二维动态规划**寻找最优分割：

1. **构建图**：词中每个位置到位置的子词构成边
2. **动态规划**：计算到达每个位置的最优分割
3. **回溯**：从终点回溯得到最优分割序列

对于单词 $w = c_1c_2...c_n$，最优分割满足：

$$\text{best\_segmentation}(w) = \arg\max_s P(s)$$

其中 $s$ 是 $w$ 的所有可能分割。

### 训练中的损失变化

移除token时的损失变化：

$$\Delta \mathcal{L}(\text{token}) = \mathcal{L}(\text{model} \setminus \{\text{token}\}) - \mathcal{L}(\text{model})$$

算法移除 $\Delta \mathcal{L}$ 最小的token（即对损失影响最小的token）。

### 算法特点

- **概率模型**：基于Unigram语言模型进行分词
- **最优分割**：使用Viterbi算法确保找到概率最大的分割
- **损失驱动**：通过最小化语料库损失优化词汇表
- **灵活性**：不依赖合并规则，分词更加灵活

## 三种算法对比

| 特征 | BPE | WordPiece | Unigram |
|------|-----|-----------|---------|
| **训练策略** | 从小到大构建词汇表 | 从小到大构建词汇表 | 从大到小剪枝词汇表 |
| **合并准则** | 频率最高 | 似然性最大 | 损失影响最小 |
| **分词策略** | 应用合并规则 | 最长匹配 | 概率最大分割 |
| **理论基础** | 频率统计 | 语言模型似然性 | Unigram语言模型 |
| **代表模型** | GPT系列、RoBERTa | BERT系列 | T5、XLNet |

## 总结

- **BPE**：简单高效，基于频率统计，适合多语言场景
- **WordPiece**：考虑语言模型似然性，在BERT中表现优异
- **Unigram**：理论最为完备，通过概率模型实现最优分割

这三种算法各有优势，选择哪种主要取决于具体的应用场景和模型架构需求。
